{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the OpenStreetMap data of Rohini area downloaded from mapzen.\n",
    "Open Street Map- https://www.openstreetmap.org/relation/3509673\n",
    "It is a part of the capital city of India i.e., Delhi.\n",
    "The date I downloaded the data set is 18 January 2017.\n",
    "I live here,so I am curious to work upon this dataset.\n",
    "\n",
    "The data file is in XML format, and here is a link to the description of the OpenStreetMap XML format.\n",
    "Link - http://wiki.openstreetmap.org/wiki/OSM_XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems Encountered in the Map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initially analyzing a small sample size of the Rohini area, I noticed following main problems with the data:\n",
    "\n",
    "1.\"k\" attributes (\"name\" and \"name:en\") having same values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Abbrevated Language codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Problem with Postal Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.  \"k\" attributes (\"name\" and \"name:en\") having same values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some node elements with \"tag\" as a child element, the \"k\" attributes \"name\" and \"name:en\" has same corresponding \"v\" attributes. Since, they are showing the same information, I felt that \"name:en\" values of \"k\" attrbiute are more informative. So, I decided to keep only \"name:en\" values of \"k\" attributes if they have a same value as \"name\".\n",
    "A glimpse of the problem and correction has been included in the code section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Abbrevated Language codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though, it is a best practice followed by OpenStreetMap to use language codes like en for english,hi for hindi etc. I found it confusing as I wasn't aware of this fact earlier and I didn't know all the language codes I could see in my sample file. \n",
    "So, as a learning opportunity I decided to expand the language codes as a part of cleaning process.\n",
    "A glimpse of the problem and correction has been included in the code section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Problem with postal codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A glance at the postal codes revealed some problems with the postal codes. The area that I have chosen has postal codes starting from 11 and followed by 4 numerical digits. However, there were few postal codes in my data set starting from \"2\". When I print those tags I found that don't beong to the chosen area. So, I chose to drop those child tags.\n",
    " And, a case of 5 digit postal code is also observed. It was due to some error and I corrected it to its right value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes the codes to carry out the data wrangling tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import schema\n",
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$') \n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"rohini.osm\"\n",
    "samplefile = \"sample3.osm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting the various types of tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to count tags\n",
    "def count_tags(filename):\n",
    "    tags = {} # creating an empty dictionary\n",
    "    for event, element in ET.iterparse(filename):\n",
    "        if element.tag not in tags:\n",
    "            tags[element.tag] = 1\n",
    "        else:\n",
    "            tags[element.tag] += 1\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': 1,\n",
       " 'member': 2616,\n",
       " 'nd': 364127,\n",
       " 'node': 288579,\n",
       " 'osm': 1,\n",
       " 'relation': 749,\n",
       " 'tag': 73639,\n",
       " 'way': 63031}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting number of tags\n",
    "count_tags(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Auditing and correction problems in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sub section deals includes auditing and correction of the mentioned problems in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Auditing Tag types(Problem 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the \"tag\" elements having both \"name\" and \"name:en\" attrbutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for event,element in ET.iterparse(filename):\n",
    "    if element.tag == \"tag\":\n",
    "        #checking whether \"name\" and \"name:en\" are attributes of \"tag\" element\n",
    "        '''if (\"name\" == element.attrib[\"k\"]) or (\"name:en\" == element.attrib[\"k\"]):\n",
    "            # printing \"k\" and \"v\" attributes\n",
    "             print element.attrib[\"k\"] , element.attrib[\"v\"]'''\n",
    "# Uncomment the if statement to print results                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning \"tag\" elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only \"name:en\" attributes if they have same value as \"name\" attributes otherwise keeping them both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Approach to retain only \"name:en\" as K attribute\n",
    "\n",
    "# This function will yield element if it is right type of tag\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = ET.iterparse(filename, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "            \n",
    "# This function will clean the list of names attributes\n",
    "def clean_names_2(name_attrib):\n",
    "    # if there is only one item in the list, just return it\n",
    "    if len(name_attrib) == 1:\n",
    "        return name_attrib\n",
    "    # otherwise, check if the 'v' attributes are the same\n",
    "    # if they are, select one, if not return the list\n",
    "    else:\n",
    "        if name_attrib[0][\"name\"] == name_attrib[1][\"name:en\"]:\n",
    "            return [name_attrib[1]]\n",
    "        else:\n",
    "            return name_attrib\n",
    "\n",
    "\n",
    "for element in get_element(samplefile):\n",
    "    # create an empty list to capture the name attributes\n",
    "    name_attrib = []\n",
    "    for child in element:\n",
    "        if child.tag == \"tag\":\n",
    "            # the condition on the 'k' attribute of the tag element\n",
    "            if \"name\" == child.attrib[\"k\"] or \"name:en\" == child.attrib[\"k\"]:\n",
    "                # append both the 'k' and 'v' attributes as a dictionary\n",
    "                name_attrib.append({child.attrib[\"k\"]:child.attrib[\"v\"]})\n",
    "\n",
    "    # if the list is still empty, this condition will not be met\n",
    "    # (empty lists have a truth value of 'False'), so this\n",
    "    # section will be skipped\n",
    "    '''\n",
    "    if name_attrib:\n",
    "        print \"\\n\", \"BEFORE\"\n",
    "        print name_attrib\n",
    "        # call the clean function\n",
    "        name_attrib = clean_names_2(name_attrib)\n",
    "        print \"\\n\", \"AFTER\"\n",
    "        print name_attrib\n",
    "        '''\n",
    "# Uncomment the last if statement to print results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing Types of language codes(Problem 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for event, element in ET.iterparse(filename, events=(\"start\",)):\n",
    "    # looking for node and relation elements\n",
    "    if element.tag == \"node\" or element.tag == \"relation\":\n",
    "        #looking if they have \"tag\" as a child element\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            # searching for \"name:language codes\" and skipping \"name:en\" as we have seen such pairs above\n",
    "            '''\n",
    "            if re.search(\"name:\", tag.attrib[\"k\"]) and \"name:en\" not in tag.attrib[\"k\"]:\n",
    "                print tag.attrib[\"k\"]\n",
    "                '''\n",
    "# Uncomment the last if statement to print results            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction Strategy(Problem 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting those language codes to their expanded form using a mapping dictionary and a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A dictionary for language codes\n",
    "mapping = { \"name:en\": \"name:english\",\n",
    "            \"name:hi\": \"name:hindi\",\n",
    "            \"name:de\" : \"name:German\",\n",
    "            \"name:ja\" : \"name:japanese\",\n",
    "            \"name:ko\" : \"name:korean\",\n",
    "            \"name:ru\" : \"name:Russian\",\n",
    "           \"name:kn\" : \"name:kannada\",\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to expand the language codes\n",
    "def update_name(name, mapping):\n",
    "    #Searching for \"k\" attribut in mapping dictionary\n",
    "    if name in mapping:      \n",
    "        #replacing code with expanded form\n",
    "        name = mapping[name]       \n",
    "        return name \n",
    "    else:\n",
    "      return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looping over the node and relation element to apply the correction.\n",
    "for event, element in ET.iterparse(filename, events=(\"start\",)):\n",
    "    if element.tag == \"node\" or element.tag == \"relation\":\n",
    "        #Searching for \"tag\" as child element\n",
    "        for tag in element.iter(\"tag\"):\n",
    "         '''  \n",
    "            if 'name:' in tag.attrib['k']:\n",
    "                #Calling function on \"k\" attribute\n",
    "                tag.attrib['k'] = update_name(tag.attrib['k'], mapping)\n",
    "                print tag.attrib[\"k\"]\n",
    "                '''\n",
    "# Uncomment the if statement to print results        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing postal codes(Problem 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to return the postal code attributeof tag element.\n",
    "def is_postal_name(element):\n",
    "    return (element.attrib['k'] == \"addr:postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['10089', '110026', '201301', '110052', '110042', '110088', '110089', '201010', '110085', '110086', '110035', '110034', '110063'])\n"
     ]
    }
   ],
   "source": [
    "# Storing postal codes in a set()\n",
    "postal_codes = set()\n",
    "for event, element in ET.iterparse(filename, events=(\"start\",)):\n",
    "    # looking for node and way elements with \"tag\" as their child element\n",
    "    if element.tag == \"node\" or element.tag == \"way\":\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            if is_postal_name(tag):\n",
    "                postal_codes.add(tag.attrib['v'])\n",
    "print postal_codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Postal Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for event, element in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if element.tag == \"node\" or element.tag == \"way\":\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if is_postal_name(tag):\n",
    "        # postcode \"110089\" has been incorrectly entered as\"10089\"           \n",
    "                    if tag.attrib['v'] == \"10089\":\n",
    "        # Correcting postcode to its right value\n",
    "                        tag.attrib[\"v\"] = \"110089\"\n",
    "        # postcodes starting from \"2\" are wrong so omit the postcode tag \n",
    "                    elif tag.attrib['v'][0] == \"2\":\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing XML file to load it into database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsection contains the code used to create the csv files from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although two problems have been identified and correction strategy has been depicted above to correct the issues with the dataset.\n",
    "Only, coorection to second problem i.e. correction for the language code has been included in the final shape function to show the skills acquired in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resulting File names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SCHEMA = schema.schema\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "OSM_PATH = filename\n",
    "PROBLEMCHARS =problemchars\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=problemchars, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for i in NODE_FIELDS:\n",
    "            node_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            tag_dict= {}\n",
    "            # Applying correction for post code in node element\n",
    "            if is_postal_name(tag):\n",
    "            # postcode \"110089\" has been incorrectly entered as\"10089\"           \n",
    "                    if tag.attrib['v'] == \"10089\":\n",
    "            # Correcting postcode to its right value\n",
    "                        tag.attrib[\"v\"] = \"110089\"\n",
    "            # postcodes starting from \"2\" are wrong so omit the postcode tag \n",
    "                    elif tag.attrib['v'][0] == \"2\":\n",
    "                        continue\n",
    "            # Calling the function to clean the language code problems\n",
    "            tag.attrib['k'] = update_name(tag.attrib['k'], mapping)\n",
    "            tag_dict['id'] = node_attribs['id']\n",
    "            key = tag.attrib['k']\n",
    "            if re.search(problemchars, tag.attrib['k']):\n",
    "                pass\n",
    "            if re.search(lower_colon, tag.attrib['k']):\n",
    "                pass\n",
    "            if ':' in tag.attrib['k']:\n",
    "                type = key[: key.index(':')]\n",
    "                key = key[key.index(':')+1 :]   \n",
    "            else:\n",
    "                type = 'regular'   \n",
    "            tag_dict['key'] = key\n",
    "            tag_dict['value'] = tag.attrib['v']\n",
    "            tag_dict['type'] = type\n",
    "            tags.append(tag_dict)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        for way in WAY_FIELDS:\n",
    "            way_attribs[way] = element.attrib[way]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            tag_dict1= {}\n",
    "             # Applying correction for post code in node element\n",
    "            if is_postal_name(tag):\n",
    "            # postcode \"110089\" has been incorrectly entered as\"10089\"           \n",
    "                    if tag.attrib['v'] == \"10089\":\n",
    "            # Correcting postcode to its right value\n",
    "                        tag.attrib[\"v\"] = \"110089\"\n",
    "            # postcodes starting from \"2\" are wrong so omit the postcode tag \n",
    "                    elif tag.attrib['v'][0] == \"2\":\n",
    "                        continue\n",
    "            tag_dict1['id'] = way_attribs['id']\n",
    "            key = tag.attrib['k']\n",
    "            if re.search(PROBLEMCHARS, tag.attrib['k']):\n",
    "                pass\n",
    "            if re.search(PROBLEMCHARS, tag.attrib['k']):\n",
    "                pass\n",
    "            if ':' in tag.attrib['k']:\n",
    "                type = key[: key.index(':')]\n",
    "                key = key[key.index(':')+1 :]\n",
    "            else:\n",
    "                type = 'regular' \n",
    "            tag_dict1['key'] = key\n",
    "            tag_dict1['value'] = tag.attrib['v']\n",
    "            tag_dict1['type'] = type\n",
    "            tags.append(tag_dict1) \n",
    "        i= 0\n",
    "        for tag in element.iter(\"nd\"):\n",
    "            way_dict = {}\n",
    "            way_dict[\"id\"] = way_attribs[\"id\"]\n",
    "            way_dict[\"node_id\"] = tag.attrib[\"ref\"]\n",
    "            way_dict[\"position\"] = i\n",
    "            way_nodes.append(way_dict)\n",
    "            i +=1    \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}    \n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following sections involves creating SQL database and running SQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing file into the sql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite_file = 'vcdb.db'    # name of the sqlite database file\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(sqlite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a cursor object\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('DROP TABLE IF EXISTS nodes')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "    CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT,type TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, working with nodes_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the csv file as a dictionary, format the\n",
    "# data as a list of tuples:\n",
    "with open('nodes_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode('utf-8'), i['key'].decode('utf-8'),i['value'].decode('utf-8'), i['type'].decode('utf-8')) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the table, specifying the column names and data types:\n",
    "cur.execute('''\n",
    "    CREATE TABLE nodes(id INTEGER, lat INTEGER, lon INTEGER, uid INGETER, version INTEGER, changeset TEXT,timestamp TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, working with nodes.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [ (i[\"id\"], i['lat'],i['lon'], i['uid'], i[\"version\"], i[\"changeset\"], i[\"timestamp\"]) for i in dr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO nodes(id,lat,lon,uid,version,changeset,timestamp) VALUES (?,?,?,?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time for ways.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the table, specifying the column names and data types:\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways(id INTEGER, user INTIGER, uid INGETER, version INTEGER, changeset TEXT,timestamp TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [ (i[\"id\"], i['user'], i['uid'], i[\"version\"], i[\"changeset\"], i[\"timestamp\"]) for i in dr]\n",
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways(id,user,uid,version,changeset,timestamp) VALUES (?,?,?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ways_tags.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "    CREATE TABLE ways_tags(id INTEGER, key TEXT, value TEXT,type TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "# Read in the csv file as a dictionary, format the\n",
    "# data as a list of tuples:\n",
    "with open('ways_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode('utf-8'), i['key'].decode('utf-8'),i['value'].decode('utf-8'), i['type'].decode('utf-8')) for i in dr]\n",
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ways_nodes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "    CREATE TABLE ways_nodes(id INTEGER,node_id INTEGER,position INTEGER)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "# Read in the csv file as a dictionary, format the\n",
    "# data as a list of tuples:\n",
    "with open('ways_nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['node_id'],i['position']) for i in dr]\n",
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways_nodes(id,node_id,position) VALUES ( ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview and Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the overview of data and SQL queries used to fetch them and some additional ideas about the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rohini.osm ......... 63.2 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vcdb.db .......... 33.2 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nodes.csv ............. 24 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nodes_tags.csv ........ 70 kB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ways.csv .............. 4 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ways_tags.csv ......... 8 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ways_nodes.cv ......... 2.5 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section various queries have been made to the database to learn about the characteristics of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a connection with the database to run queries :- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite_file = 'mydb.db'    # change this to the name of your sqlite database file\n",
    "\n",
    "# Connecting to the database file\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting number of nodes_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1918,)]\n"
     ]
    }
   ],
   "source": [
    "# Make a query \n",
    "query1 = '''SELECT COUNT(*) as count  FROM nodes_tags '''\n",
    "#Execute the query\n",
    "result1 = c.execute(query1)\n",
    "#print results\n",
    "print list(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(63031,)]\n"
     ]
    }
   ],
   "source": [
    "query2 = '''SELECT COUNT(*) as count FROM ways'''\n",
    "result2 = c.execute(query2)\n",
    "print list(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of ways_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(69828,)]\n"
     ]
    }
   ],
   "source": [
    "query3 = '''SELECT COUNT(*) as count FROM ways_tags'''\n",
    "result3 = c.execute(query3)\n",
    "print list(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of ways_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(364127,)]\n"
     ]
    }
   ],
   "source": [
    "query4 = '''SELECT COUNT(*) as count FROM ways_nodes'''\n",
    "result4 = c.execute(query4)\n",
    "print list(result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(288579,)]\n"
     ]
    }
   ],
   "source": [
    "query5 = '''SELECT COUNT(*) as count FROM nodes'''\n",
    "result5 = c.execute(query5)\n",
    "print list(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Number of Unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(156,)]\n"
     ]
    }
   ],
   "source": [
    "query = ''' SELECT COUNT(DISTINCT(uid))          \n",
    "FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways)'''\n",
    "result= c.execute(query)\n",
    "print list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top ten contributing user id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2901478, 60488), (56597, 37016), (2913195, 29729), (2897305, 20942), (2900596, 20258), (2913190, 20238), (2949621, 17751), (2907704, 14534), (2901493, 13633), (2949569, 12692)]\n"
     ]
    }
   ],
   "source": [
    "query = ''' SELECT uid, COUNT(*) as num\n",
    "FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) \n",
    "GROUP BY uid\n",
    "ORDER BY num DESC\n",
    "LIMIT 10'''\n",
    "result= c.execute(query)\n",
    "print list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Users with sinlge contribution only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(34,)]\n"
     ]
    }
   ],
   "source": [
    "query= ''' SELECT COUNT(*) \n",
    "FROM\n",
    "    (SELECT uid, COUNT(*) as num\n",
    "     FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) \n",
    "     GROUP BY uid\n",
    "     HAVING num=1) '''\n",
    "result= c.execute(query)\n",
    "print list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Most popular Ammenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'fuel', 19), (u'school', 19), (u'place_of_worship', 17), (u'restaurant', 15), (u'bank', 13), (u'hospital', 8), (u'clinic', 6), (u'marketplace', 5), (u'parking', 5), (u'atm', 4)]\n"
     ]
    }
   ],
   "source": [
    "query = ''' SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key='amenity'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10'''\n",
    "result= c.execute(query)\n",
    "print list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Popular Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'hindu', 4), (u'sikh', 2)]\n"
     ]
    }
   ],
   "source": [
    "query= ''' SELECT nodes_tags.value, COUNT(*) as num\n",
    "FROM nodes_tags \n",
    "    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='place_of_worship') i\n",
    "    ON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key='religion'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC\n",
    "LIMIT 20'''\n",
    "result= c.execute(query)\n",
    "print list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Favourite Cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'burger', 2), (u'pizza', 2), (u'chicken_burger', 1), (u'indian', 1), (u'south indian', 1)]\n"
     ]
    }
   ],
   "source": [
    "query = ''' SELECT nodes_tags.value, COUNT(*) as num\n",
    "FROM nodes_tags \n",
    "    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='restaurant') i\n",
    "    ON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key='cuisine'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC'''\n",
    "result= c.execute(query)\n",
    "print list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most popular Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'State Bank of India', 3), (u'ICICI Bank', 2), (u'HDFC bank', 1), (u'HSBC', 1), (u'ICICI', 1)]\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT nodes_tags.value, COUNT(*) as num\n",
    "        FROM nodes_tags\n",
    "            JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='bank') i\n",
    "            ON nodes_tags.id=i.id\n",
    "        WHERE nodes_tags.key='name'\n",
    "        GROUP BY nodes_tags.value\n",
    "        ORDER BY num DESC\n",
    "        LIMIT 5'''\n",
    "result= c.execute(query)\n",
    "print list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Suggestions for improvement of analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still several opportunities for cleaning and validation that I left unexplored. Of note, the data set is populated only from one source: OpenStreetMaps. While this crowdsourced repository pulls from multiple sources, some of data is potentially outdated. \n",
    "It would have been an interesting exercise to validate and/or pull missing information (i.e. names) from the Google Maps API, since every node has latitude-longitude coordinates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis, I was interested to perform the analysis of data set of complete city Delhi. However, due to huge file size and very poor PC performance, I had to limit my analysis to a very small part i.e. Rohini, where I live in.\n",
    "But, During the analysis, I thought that I would be cool to analyse and create database from various parts of the city and do a comparative study. For ex. we can look for particular type of cuisine restraunts or malls concentration in various part of Delhi and that study can help us to make a machine learning algorithm that can predict whether a particular type of shop/restraunt can be opened in a place or not. \n",
    "I would really like to use all the skills learnt in udacity to come up with this model. Though, the data has to be backed by some additional information like purchasing power of people in that area etc. So, there are a lot of difficulties to handle too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
